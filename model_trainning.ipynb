{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6abdc86a-776b-491d-8d75-d88dad2cdb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1ad43d4-fa72-494c-af0f-3ab6cca19229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01, epochs=500):\n",
    "        print(\"Initialisation du NeuralNetwork...\")\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size) * 0.1\n",
    "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
    "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size) * 0.1\n",
    "        self.bias_output = np.zeros((1, self.output_size))\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def softmax_derivative(self, z):\n",
    "        return z * (1 - z)\n",
    "\n",
    "    def fit(self, X, y, batch_size=32):\n",
    "        print(\"Début de l'entraînement du NeuralNetwork...\")\n",
    "        y_one_hot = np.eye(self.output_size)[y]\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            if epoch % 50 == 0:\n",
    "                print(f\"Époque {epoch}/{self.epochs}\")\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X[i:i + batch_size]\n",
    "                y_batch = y_one_hot[i:i + batch_size]\n",
    "\n",
    "                hidden_layer_activation = np.dot(X_batch, self.weights_input_hidden) + self.bias_hidden\n",
    "                hidden_layer_output = self.softmax(hidden_layer_activation)\n",
    "                final_layer_activation = np.dot(hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
    "                output = self.softmax(final_layer_activation)\n",
    "\n",
    "                error = y_batch - output\n",
    "                d_output = error\n",
    "\n",
    "                d_hidden_layer = np.dot(d_output, self.weights_hidden_output.T) * self.softmax_derivative(hidden_layer_output)\n",
    "\n",
    "                self.weights_hidden_output += np.dot(hidden_layer_output.T, d_output) * self.learning_rate\n",
    "                self.bias_output += np.sum(d_output, axis=0, keepdims=True) * self.learning_rate\n",
    "                self.weights_input_hidden += np.dot(X_batch.T, d_hidden_layer) * self.learning_rate\n",
    "                self.bias_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        hidden_layer_activation = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        hidden_layer_output = self.softmax(hidden_layer_activation)\n",
    "        final_layer_activation = np.dot(hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
    "        output = self.softmax(final_layer_activation)\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=500):\n",
    "        print(\"Initialisation de la LogisticRegression...\")\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print(\"Début de l'entraînement de la LogisticRegression...\")\n",
    "        self.weights = np.zeros((X.shape[1], len(np.unique(y))))\n",
    "        self.bias = np.zeros((1, len(np.unique(y))))\n",
    "        m = len(y)\n",
    "        y_one_hot = np.eye(len(np.unique(y)))[y]\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            if epoch % 50 == 0:\n",
    "                print(f\"Époque {epoch}/{self.epochs}\")\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            predictions = self.softmax(linear_model)\n",
    "\n",
    "            dw = (1 / m) * np.dot(X.T, (predictions - y_one_hot))\n",
    "            db = (1 / m) * np.sum(predictions - y_one_hot, axis=0, keepdims=True)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        predictions = self.softmax(linear_model)\n",
    "        return np.argmax(predictions, axis=1)\n",
    "\n",
    "class XGBoost:\n",
    "    def __init__(self, learning_rate=0.1, n_estimators=100, max_depth=3):\n",
    "        print(\"Initialisation de XGBoost...\")\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print(\"Début de l'entraînement de XGBoost...\")\n",
    "        y_one_hot = np.eye(len(np.unique(y)))[y]\n",
    "        residual = y_one_hot.copy()\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            model = LogisticRegression(learning_rate=self.learning_rate, epochs=100)\n",
    "            model.fit(X, np.argmax(residual, axis=1))\n",
    "            predictions = model.predict(X)\n",
    "            predictions_one_hot = np.eye(len(np.unique(y)))[predictions]\n",
    "            residual -= predictions_one_hot\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], len(self.models[0].weights[0])))\n",
    "        for model in self.models:\n",
    "            pred = model.predict(X)\n",
    "            predictions += np.eye(len(predictions[0]))[pred]\n",
    "        return np.argmax(predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58cfc084-c228-4986-aa35-4a77f817aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_hyperparameter_search(X, y):\n",
    "    print(\"Début de la recherche manuelle des hyperparamètres...\")\n",
    "\n",
    "    # Liste des classes uniques\n",
    "    unique_classes = np.unique(y)\n",
    "    num_classes = len(unique_classes)\n",
    "\n",
    "    # Création d'un mapping entre étiquettes réelles et indices\n",
    "    class_to_index = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "    index_to_class = {idx: cls for cls, idx in class_to_index.items()}\n",
    "\n",
    "    # Conversion des étiquettes en indices pour one-hot encoding\n",
    "    y_indices = np.array([class_to_index[label] for label in y])\n",
    "\n",
    "    # Division des données\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(f\"Train size: {len(X_train)}, Validation size: {len(X_val)}\")\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_model = None\n",
    "    best_conf_matrix = None\n",
    "\n",
    "    # Recherche pour les réseaux de neurones\n",
    "    for learning_rate in [0.01, 0.05, 0.1]:\n",
    "        for epochs in [50, 100, 150]:\n",
    "            for hidden_size in [10, 20, 30]:\n",
    "                model = NeuralNetwork(\n",
    "                    input_size=X_train.shape[1],\n",
    "                    hidden_size=hidden_size,\n",
    "                    output_size=num_classes,\n",
    "                    learning_rate=learning_rate,\n",
    "                    epochs=epochs,\n",
    "                )\n",
    "                model.fit(X_train, y_train)\n",
    "                f1, conf_matrix = evaluate_model_with_mapping(model, X_val, y_val, index_to_class)\n",
    "\n",
    "                print(f\"Testé: lr={learning_rate}, epochs={epochs}, hidden_size={hidden_size} => F1={f1}\")\n",
    "\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_model = model\n",
    "                    best_conf_matrix = conf_matrix\n",
    "\n",
    "    # Recherche pour XGBoost\n",
    "    for learning_rate in [0.1, 0.2]:\n",
    "        for n_estimators in [50, 100]:\n",
    "            model = XGBoost(learning_rate=learning_rate, n_estimators=n_estimators, max_depth=3)\n",
    "            model.fit(X_train, y_train)\n",
    "            f1, conf_matrix = evaluate_model_with_mapping(model, X_val, y_val, index_to_class)\n",
    "\n",
    "            print(f\"Testé: lr={learning_rate}, n_estimators={n_estimators} => F1={f1}\")\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model = model\n",
    "                best_conf_matrix = conf_matrix\n",
    "\n",
    "    print(\"Recherche manuelle terminée.\")\n",
    "    print(\"Meilleur F1:\", best_f1)\n",
    "    print(\"Matrice de confusion finale :\\n\", best_conf_matrix)\n",
    "\n",
    "    return best_model, best_f1, best_conf_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
