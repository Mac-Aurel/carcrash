{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9cdaefb-bb6f-4cf3-90e7-8ca814aa3403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(X, y, threshold=0.1):\n",
    "    \"\"\"Filtre les colonnes numériques basées sur leur corrélation avec la cible.\"\"\"\n",
    "    correlations = {}\n",
    "    for col in X.select_dtypes(include=[np.number]).columns:\n",
    "        corr = np.corrcoef(X[col], y)[0, 1]\n",
    "        correlations[col] = corr\n",
    "    selected = [col for col, corr in correlations.items() if abs(corr) > threshold]\n",
    "    print(f\"Colonnes numériques retenues (corrélation > {threshold}): {selected}\")\n",
    "    return selected\n",
    "\n",
    "def categorical_analysis(X, y, threshold=0.2):\n",
    "    \"\"\"Filtre les colonnes catégorielles en se basant sur la variance des moyennes.\"\"\"\n",
    "    X_with_target = X.copy()\n",
    "    X_with_target['target'] = y\n",
    "\n",
    "    selected = []\n",
    "    for col in X.select_dtypes(include='object').columns:\n",
    "        means = X_with_target.groupby(col)['target'].mean()\n",
    "        if means.var() > threshold:\n",
    "            selected.append(col)\n",
    "    print(f\"Colonnes catégorielles retenues (variance > {threshold}): {selected}\")\n",
    "    return selected\n",
    "\n",
    "def low_variance_filter(X, threshold=0.01):\n",
    "    \"\"\"Filtre les colonnes numériques avec une faible variance.\"\"\"\n",
    "    X_numeric = X.select_dtypes(include=[np.number])\n",
    "    variances = X_numeric.var()\n",
    "    selected = variances[variances > threshold].index.tolist()\n",
    "    print(f\"Colonnes numériques retenues (variance > {threshold}): {selected}\")\n",
    "    return selected\n",
    "\n",
    "\n",
    "def auto_handle_nan(df, nan_threshold_delete=0.5, nan_threshold_impute=0.1):\n",
    "    \"\"\"\n",
    "    Traite automatiquement les valeurs NaN dans un dataset.\n",
    "    - Supprime les colonnes avec trop de NaN.\n",
    "    - Impute (remplace) les NaN avec des stratégies adaptées :\n",
    "      - Moyenne pour colonnes numériques.\n",
    "      - Mode ou \"Inconnu\" pour colonnes catégorielles.\n",
    "    \"\"\"\n",
    "    print(\"Analyse des NaN dans le dataset...\\n\")\n",
    "    df = df.replace(-1, 1)\n",
    "    nan_percent = df.isnull().mean()\n",
    "    print(\"Pourcentage de valeurs manquantes par colonne :\")\n",
    "    print(nan_percent)\n",
    "    \n",
    "    cols_to_delete = nan_percent[nan_percent > nan_threshold_delete].index\n",
    "    print(f\"\\nColonnes supprimées (trop de NaN > {nan_threshold_delete*100}%): {list(cols_to_delete)}\")\n",
    "    df = df.drop(columns=cols_to_delete)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        missing = df[col].isnull().sum()\n",
    "        if missing > 0:\n",
    "            if df[col].dtype == 'object':\n",
    "                if nan_percent[col] > nan_threshold_impute:\n",
    "                    print(f\"Colonne '{col}' : Imputation avec 'Manquant' (catégorielle)\")\n",
    "                    df[col] = df[col].fillna(\"Manquant\")\n",
    "                else:\n",
    "                    print(f\"Colonne '{col}' : Imputation avec la valeur la plus fréquente (mode)\")\n",
    "                    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "            else:\n",
    "                if nan_percent[col] > nan_threshold_impute:\n",
    "                    print(f\"Colonne '{col}' : Imputation avec la médiane (numérique)\")\n",
    "                    df[col] = df[col].fillna(df[col].median())\n",
    "                else:\n",
    "                    print(f\"Colonne '{col}' : Imputation avec la moyenne (numérique)\")\n",
    "                    df[col] = df[col].fillna(df[col].mean())\n",
    "    \n",
    "    print(\"\\nTraitement des NaN terminé.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f0c899d-ca1a-4bbe-9a40-dd8be5a32093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    print(\"Division des données en ensembles d'entraînement et de validation...\")\n",
    "    if random_state:\n",
    "        np.random.seed(random_state)\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split_idx = int(len(X) * (1 - test_size))\n",
    "    train_indices = indices[:split_idx]\n",
    "    test_indices = indices[split_idx:]\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model_with_mapping(model, X_val, y_val, index_to_class):\n",
    "    # Prédictions en indices\n",
    "    predictions_indices = model.predict(X_val)\n",
    "\n",
    "    # Conversion des indices de prédiction et de validation en étiquettes réelles\n",
    "    predictions = np.array([index_to_class[idx] for idx in predictions_indices])\n",
    "    y_val_actual = np.array([index_to_class[idx] for idx in y_val])\n",
    "\n",
    "    # Évaluation avec les vraies étiquettes\n",
    "    f1 = f1_score_manual(y_val_actual, predictions)\n",
    "    conf_matrix = confusion_matrix_manual(y_val_actual, predictions)\n",
    "\n",
    "    return f1, conf_matrix\n",
    "\n",
    "\n",
    "def f1_score_manual(y_true, y_pred):\n",
    "    f1_scores = []\n",
    "    for label in np.unique(y_true):\n",
    "        tp = np.sum((y_true == label) & (y_pred == label))\n",
    "        fp = np.sum((y_true != label) & (y_pred == label))\n",
    "        fn = np.sum((y_true == label) & (y_pred != label))\n",
    "        if tp + fp == 0 or tp + fn == 0:\n",
    "            f1_scores.append(0)\n",
    "        else:\n",
    "            precision = tp / (tp + fp)\n",
    "            recall = tp / (tp + fn)\n",
    "            f1_scores.append(2 * (precision * recall) / (precision + recall))\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "def confusion_matrix_manual(y_true, y_pred):\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    conf_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        conf_matrix[t, p] += 1\n",
    "    return conf_matrix\n",
    "\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    predictions = model.predict(X_val)\n",
    "    print(\"Prédictions :\", predictions[:10])\n",
    "    print(\"Vraies valeurs :\", y_val[:10])\n",
    "    f1 = f1_score_manual(y_val, predictions)\n",
    "    conf_matrix = confusion_matrix_manual(y_val, predictions)\n",
    "    return f1, conf_matrix\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
